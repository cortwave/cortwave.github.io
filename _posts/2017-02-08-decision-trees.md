---
layout: post
title: "Деревья решений"
description: "Один из наиболее простых для понимания, но, тем не менее, часто применяемых алгоритмов машинного обучения"
category: data 
tags: ['ml', 'data', 'sklearn', 'python', 'decision tree']
---

[Деревья принятия решений](https://ru.wikipedia.org/wiki/%D0%94%D0%B5%D1%80%D0%B5%D0%B2%D0%BE_%D0%BF%D1%80%D0%B8%D0%BD%D1%8F%D1%82%D0%B8%D1%8F_%D1%80%D0%B5%D1%88%D0%B5%D0%BD%D0%B8%D0%B9) - алгоритм машинного обучения, который чаще всего применяется для решения задач классификации, но также вполне успешно может быть применен и для решения задачи регрессии. На самом деле, многие люди мыслят такими деревьями, не осознавая этого. Суть дерева принятия решений состоит в том, что на каждом этапе мы задаем себе вопрос и в зависимости от ответа двигаемся к следующему вопросу или же, если уже получено достаточное количество таких ответов, выдаем решение. Вот [пример документации](http://xgrommx.github.io/rx-book/content/which_operator_do_i_use/creation_operators.html) неплохо демонстрирующий дерево принятия решений (хотя и не бинарное). Для того, чтобы выбрать, какой метод из библиотеки RX вызвать, пользователь должен ответить на серию вопросов, после ответа на каждый вопрос множество вариантов (методов) сужается. Таким обрaзом, дерево решений представляет из себя дерево (чаще всего бинарное), в вершинах которого содержатся предикаты, по которым приходящая в данную вершину выборка, делится на группы (вопросы), а в листьях - результат классификации (результат работы алгоритма).

{: .center }
![Umbrella tree](/images/umbrella.png){: .center-image }
Рис. 1 Дерево решений для взятия зонта

Одним из преимуществ данного метода является простая интерпретация результата классификации, в следствие чего результат работы алгоритма может быть пояснен человеку далекому от машинного обучения. При желании, на основе построенного дерева решений можно сгенерировать текст, поясняющий результат его работы. Например, для задачи кредитного скоринга результат классификации клиента между классами 'платежеспособный', 'неплатежеспособный' может быть интерпретирован так: "Клиент является платежеспособным, так как имеет более 2-ух погашенных кредитов в нашем банке, его возраст более 30 лет, заработная плата > 2.000$/месяц". Подобного результата было бы сложно добиться с нейронной сетью.

### Применение

Применим алгоритм дерева решений на практике. На сайте [Kaggle](https://kaggle.com) размещено пробное соревнование [Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic), основной целью которого является познакомить людей, только начинающих заниматься машинным обучением, с принципом проведения таких соревнований. Часть связанную именно с Kaggle мы опустим, так как нас интересует только набор данных из этого соревнования. Данные можно найти [здесь](https://www.kaggle.com/c/titanic/data). Каждая строка csv-файла представляет из себя набор признаков пассажира (пол, возраст, цена билета и т.д. - более подробное описание данных можно найти на странице их скачивания). На основе этих данных требуется предсказать, выжил ли данный пассажир.

Для построения модели я буду использовать библиотеку [Scikit-learn](http://scikit-learn.org/stable/). Дерево решений умеет работать только с численными признаками, поэтому категориальные признаки кодируем по принципу [one-hot encoding](https://ru.wikipedia.org/wiki/%D0%A3%D0%BD%D0%B8%D1%82%D0%B0%D1%80%D0%BD%D1%8B%D0%B9_%D0%BA%D0%BE%D0%B4), а текстовые признаки (имя пассажира, номер билета, номер кабины) удалим из выборки, так как пока не знаем, как их правильно представить в численном виде. Пропущенные значения заменим средними значениями.

```python
import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score
from sklearn import tree

train = pd.read_csv("train.csv")
train = train.drop(['Name', 'Cabin', 'Ticket', 'PassengerId'], axis=1)

# заполняем пропущенные значения средними
train = train.fillna(np.mean(train))
# one-hot encoding
train = pd.get_dummies(train)

# Разделим тренировочную выборку на обучаюшую и выборку для валидации в отношении 70/30.
msk = np.random.rand(len(train)) < 0.7 
cv = train[~msk] 
train = train[msk]

#Разделим наши выборки на признаки и метки.
def features(data):
    return data.drop(['Survived'], axis = 1, errors='ignore')

def labels(data):
    return data.Survived

cv_features = features(cv) 
cv_labels = labels(cv)
train_features = features(train) 
train_labels = labels(train)

#Построим модель, обучим ее, и протестируем на валидационной выборке.
tr = tree.DecisionTreeClassifier()
tr = tr.fit(train_features, train_labels)

prediction = tr.predict(cv_features)
accuracy_score(y_pred=prediction, y_true=cv_labels)
```

Точность получившейся классификации 0.795 (при разных запусках будет меняться, так как деление на обучающую и валидационную выборки содержит элемент случайности). Является ли данный результат достаточно хорошим? Для того чтобы ответить на этот вопрос визуализируем построенное дерево решений.

```python
from graphviz import Digraph
from subprocess import check_call

tree.export_graphviz(tr, out_file = "dtree.dot", feature_names = train_features.columns)
check_call(['dot','-Tpng','dtree.dot','-o','dtree.png'])
```

В моем случае файл dtree.png содержал [следующее изображение](/images/titanic_tree_1.png).

Дерево получилось достаточно большим, что может натолкнуть на мысль о переобучении нашей модели (модель подстроилась под специфические шумы данной выборки, которые навряд ли будут проявляться в других выборках). Для того, чтобы убедиться в этом посмотрим на вершины дерева, имеющие наибольшую глубину:

{: .center }
![titanic tree part](/images/tree_part.png){: .center-image height="600px" width="600px"}
Рис. 2 Вершины дерева решений с глубиной ~ 18

В первой строке вершины мы можем наблюдать предикат, по которому это вершина делит приходящую в нее выборку. В следующей строке - значение коеффициента, вычисленного по [правилу Джини](https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity)(данное правило используется для выбора признака, по которому создается предикат, и граничного значения данного предиката). В третьей строке можно увидеть количество наблюдений тренировочной выборки, которые доходят до данной вершины. В последней строке - информация о том, сколько наблюдений какого класса пришло в данную вершину. О переобучении можно судить по малой выборке (2-4 наблюдения), которая доходит до последних вершин. В примере выше в последней вершине находится предикат по возрасту с пороговым значением 29.4406. Однако до данного предиката из тренировочной выборки дошло всего три наблюдения, поэтому закономерность, которую он создает можно считать скорее шумом, чем реальной закономерностью (все мы знаем, что строить график по 2-3 точкам не совсем правильно).

Дерево получилось достаточно большим из-за гиперпараметров модели по-умолчанию. Попробуем уменьшить дерево, установив его максимальную глубину равной 3-ем.

```python
tr = tree.DecisionTreeClassifier(max_depth=3)
```

Результат тестирования на валидационной выборке - 0.843. Уже куда лучше. Посмотрим на получившееся дерево:

{: .center }
![titanic tree with depth 3](/images/tree_3.png){: .center-image height="600px" width="1000px"}
Рис. 3 Дерево с максимальной глубиной - 3

Для того, чтобы примерно понять влияние глубины дерева на процесс переобучения, построим график зависимости точности классификации от глубины:

{: .center }
![accuracy on depth](/images/depth_accuracy.png){: .center-image height="400px" width="600px"}
Рис. 4 Зависимость точности классификации от глубины дерева

Кроме глубины дерева, в пакете sklearn его размер можно ограничить максимальным количеством листьев, минимальным значением коэффициента, по которому выбирается предикат (в нашем случае - правило Джини) и др.

Также для оценки адекватности модели можно проанализировать, какой признак внес какой вклад в классификацию. 

```python
importance = tr.feature_importances_
```

На основе вклада признаков в модель построим гистограмму:

{: .center }
![features importance](/images/tree_importance.png){: .center-image height="400px" width="600px"}
Рис. 5 Вклад различных признаков в модель

По гистограмме можно увидеть, что самым главным признаком, определяющим выживаемость являлся пол (классифицировав всю выборку по одному лишь полу можну получить точность ~75%). После пола примерно одинаковое значение имели: стоимость билета, класс пассажира, возраст пассажира. Пожалуй, странным является только более значительный вклад порта посадки на корабль, чем количество родственников на корабле. Возможно, данный параметр вляется шумовым и свойственен только тренировочной выборке. Узнать это можно, сравнив результаты [кросс-валидации](http://www.machinelearning.ru/wiki/index.php?title=%D0%9A%D1%80%D0%BE%D1%81%D1%81-%D0%B2%D0%B0%D0%BB%D0%B8%D0%B4%D0%B0%D1%86%D0%B8%D1%8F) с признаком порта посадки и без.

### Вывод

Дервья решений являются простым для понимания алгоритмом машинного обучения. Саму модель достаточно легко визуализировать и оценить ее корректность (даже на интуитивном уровне). Безусловно данную модель можно использовать как [черный ящик](https://ru.wikipedia.org/wiki/%D0%A7%D1%91%D1%80%D0%BD%D1%8B%D0%B9_%D1%8F%D1%89%D0%B8%D0%BA), но понимание принципов работы может принести значительную пользу. Также для увелечения эффективности работы модели можно использовать ее [ансамбль](https://ru.wikipedia.org/wiki/Random_forest), в котором деревья могут строиться как независимо, так и последовательно улучшая друг друга ([бустинг](http://www.machinelearning.ru/wiki/index.php?title=%D0%91%D1%83%D1%81%D1%82%D0%B8%D0%BD%D0%B3), н-р [xgboost](http://xgboost.readthedocs.io/en/latest/)).
{% include JB/setup %}
